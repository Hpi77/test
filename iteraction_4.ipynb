{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38499f17",
   "metadata": {},
   "source": [
    "DU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4f0e8",
   "metadata": {},
   "source": [
    "2.1 Collect initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744643b",
   "metadata": {},
   "source": [
    "import spark package for data mining and read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, trim, lower, split, count, avg, format_number,isnan,sum as _sum\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import (StructField, StringType, IntegerType, StructType, FloatType, DoubleType)\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, PCA, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('iteraction_4')\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read CSV document\n",
    "df1 = spark.read.csv('weather.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('fires.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8173713",
   "metadata": {},
   "source": [
    "2.2 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print DataFrame\n",
    "print(\"Weather DataFrame:\")\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "# print the shape of DataFrame\n",
    "print(\"Shape df1: ({}, {})\".format(df1.count(), len(df1.columns)))\n",
    "# print the datatype of DataFrame\n",
    "print(\"df1 dtype:\", df1.dtypes)\n",
    "\n",
    "print(\"Fires DataFrame:\")\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "print(\"Shape df2: ({}, {})\".format(df2.count(), len(df2.columns)))\n",
    "print(\"df2 dtype:\", df2.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1440bc6",
   "metadata": {},
   "source": [
    "2.3 Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588127c",
   "metadata": {},
   "source": [
    "Find missing data,count missing rows by country and identify rows containing at least one missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing value\n",
    "df1.select([(col(c).isNull().cast(\"int\")).alias(c) for c in df1.columns]).agg(*[ _sum(c).alias(c) for c in df1.columns ]).show()\n",
    "df2.select([(col(c).isNull().cast(\"int\")).alias(c) for c in df2.columns]).agg(*[ _sum(c).alias(c) for c in df2.columns ]).show()\n",
    "\n",
    "# integret DataFrame 1 and DataFrame 2\n",
    "df3 = df1.join(df2, on=['X', 'Y', 'num', 'country'], how='outer')\n",
    "\n",
    "# print datatype of integreted DataFrame(df3)\n",
    "print(\"df3 dtype:\", df3.dtypes)\n",
    "\n",
    "# count the final rows per country\n",
    "total_rows_per_country = df3.groupBy(\"country\").agg(count(\"*\").alias(\"total_count\"))\n",
    "condition = reduce(lambda x, y: x | y, [col(c).isNull() for c in df3.columns])\n",
    "missing_rows_per_country = df3.filter(condition).groupBy(\"country\").agg(count(\"*\").alias(\"missing_count\"))\n",
    "\n",
    "# count missing rate\n",
    "missing_ratio_per_country = missing_rows_per_country.join(\n",
    "    total_rows_per_country,\n",
    "    on=\"country\",\n",
    "    how=\"inner\"\n",
    ").withColumn(\n",
    "    \"missing_ratio\",\n",
    "    col(\"missing_count\") / col(\"total_count\")\n",
    ").select(\n",
    "    \"country\",\n",
    "    \"missing_ratio\"\n",
    ").orderBy(col(\"missing_ratio\").desc())\n",
    "\n",
    "# show result\n",
    "missing_ratio_per_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8da6ab",
   "metadata": {},
   "source": [
    "2.4 Verify the data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1b6d0",
   "metadata": {},
   "source": [
    "Visualise the rate of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b716d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean the data, and format the values of\"month\" and \"day\"\n",
    "df3 = df3.withColumn(\"month\", \n",
    "    when(col(\"month\") == 'jan', 1).when(col(\"month\") == 'feb', 2).when(col(\"month\") == 'mar', 3)\n",
    "    .when(col(\"month\") == 'apr', 4).when(col(\"month\") == 'may', 5).when(col(\"month\") == 'jun', 6)\n",
    "    .when(col(\"month\") == 'jul', 7).when(col(\"month\") == 'aug', 8).when(col(\"month\") == 'sep', 9)\n",
    "    .when(col(\"month\") == 'oct', 10).when(col(\"month\") == 'nov', 11).when(col(\"month\") == 'dec', 12)\n",
    "    .otherwise(col(\"month\"))\n",
    ")\n",
    "\n",
    "df3 = df3.withColumn(\"day\", \n",
    "    when(col(\"day\") == 'mon', 1).when(col(\"day\") == 'tue', 2).when(col(\"day\") == 'wed', 3)\n",
    "    .when(col(\"day\") == 'thu', 4).when(col(\"day\") == 'fri', 5).when(col(\"day\") == 'sat', 6)\n",
    "    .when(col(\"day\") == 'sun', 7).otherwise(col(\"day\"))\n",
    ")\n",
    "df3 = df3.withColumn(\"country\", \n",
    "    when(col(\"country\") == 'Portgual', 1).when(col(\"country\") == 'Brazil', 2).otherwise(col(\"country\"))\n",
    ")\n",
    "\n",
    "# 计算每个月的记录数\n",
    "monthly_counts = df3.groupBy(\"month\").agg(count(\"area\").alias(\"area\"))\n",
    "\n",
    "# use median values fill the missing values\n",
    "numeric_columns = [field.name for field in df3.schema.fields if isinstance(field.dataType, (DoubleType, IntegerType, FloatType))]\n",
    "median_values = df3.approxQuantile(numeric_columns, [0.5], 0.25)\n",
    "median_dict = {col: median_values[i] for i, col in enumerate(numeric_columns)}\n",
    "\n",
    "median_dict = {col: median_values[0][i] for i, col in enumerate(numeric_columns) if i < len(median_values[0])}\n",
    "\n",
    "df4 = df3.fillna(median_dict)\n",
    "\n",
    "df4.show()\n",
    "print(\"Statistical Description:\")\n",
    "df4.describe().show()\n",
    "\n",
    "# get the shape of DataFrame的形状\n",
    "shape = (df4.count(), len(df4.columns))\n",
    "print(\"Shape:\", shape)\n",
    "\n",
    "# count the number of missing value\n",
    "df4.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df4.columns]).show()\n",
    "\n",
    "# 验证数据类型\n",
    "print(\"Data Types:\", df4.dtypes)\n",
    "\n",
    "# 计算相关矩阵\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df4.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df4).select(vector_col)\n",
    "matrix = Correlation.corr(df_vector, vector_col).collect()[0][0]\n",
    "corr_matrix = matrix.toArray()\n",
    "\n",
    "print(\"Correlation Matrix:\", corr_matrix)\n",
    "\n",
    "# 将相关矩阵转换为DataFrame并显示\n",
    "corr_df = spark.createDataFrame(\n",
    "    corr_matrix.tolist(),\n",
    "    df4.columns\n",
    ")\n",
    "\n",
    "corr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将结果转换为pandas DataFrame以进行可视化\n",
    "monthly_counts_pd = monthly_counts.toPandas()\n",
    "df3_pd = df3.toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from functools import reduce\n",
    "\n",
    "# 启用内联绘图模式\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置绘图的默认参数\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "# Records\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(monthly_counts_pd['month'], monthly_counts_pd['area'], color='skyblue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.title('Relationship Between the Number of Area Records and Month')\n",
    "plt.show()\n",
    "\n",
    "# Relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df3_pd['month'], df3_pd['area'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Area')\n",
    "plt.title('Relationship between area and month')\n",
    "plt.show()\n",
    "\n",
    "# Distribution Plot\n",
    "sns.distplot(df3_pd['area'])\n",
    "plt.title('Distribution of Area')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40123d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
